{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Dengue Fever Cases using weather data via Catboost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project aims to tackle the Dengue Fever Challenge posted on Driven Data.\n",
    "Five steps are included in this markdown:\n",
    "1. importing packages and reading data from source\n",
    "2. conducting basic EDA (Exploration Data Analysis)\n",
    "3. feature engineering\n",
    "4. train a model and test result simply using training data\n",
    "5. train the model again using all training data and do prediction for the 'submit' set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: importing packages and reading data from source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "path = os.getcwd() + \"/Data/\"\n",
    "\n",
    "data = {}\n",
    "for file in os.listdir(path):\n",
    "    data[re.sub(\".csv\", \"\", file)] = pd.read_csv(path + file)\n",
    "\n",
    "training_X = data.pop(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Features\")\n",
    "training_Y = data.pop(\"DengAI_Predicting_Disease_Spread_-_Training_Data_Labels\")\n",
    "submit_X = data.pop(\"DengAI_Predicting_Disease_Spread_-_Test_Data_Features\")\n",
    "submit_Y = data.pop(\"DengAI_Predicting_Disease_Spread_-_Submission_Format\")\n",
    "\n",
    "# joining data for train_test_split in the later stage\n",
    "training = training_X.merge(training_Y, on=['city', 'year', 'weekofyear'], how='left')\n",
    "submit = submit_X.merge(submit_Y, on=['city', 'year', 'weekofyear'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: conduct basic EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of training data\n",
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of submission data\n",
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the training data\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the submission data\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes of each column\n",
    "training.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary of the training data\n",
    "training.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary of the submission data\n",
    "submit.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any NA in training data\n",
    "training.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any NA in submission data\n",
    "submit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe histogram of each numeric feature\n",
    "plt.figure()\n",
    "training.hist(figsize=(20,20), layout=(8,3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore correlation between variables\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(training.corr(), xticklabels=training.corr().columns, yticklabels=training.corr().columns, center=0, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: feature engineering\n",
    "\n",
    "Based on the exploratory data analysis, it seems that the following steps could/should be done:\n",
    "1. generating datetime features\n",
    "2. filling NAs\n",
    "3. removing highly correlated features\n",
    "4. one-hot encoding for categorical features\n",
    "\n",
    "For step 2, since the NA values for each column are weather related data, it makes sense to fillna in groups of city and month.\n",
    "And as the NA values only accounts for less than 10% of the total data for each column, a naive method, median would be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating date_time features\n",
    "training['week_start_date'] = pd.to_datetime(training['week_start_date'], format='%Y-%m-%d')\n",
    "training['quarter'] = training.week_start_date.dt.quarter\n",
    "training['month'] = training.week_start_date.dt.month\n",
    "training['day'] = training.week_start_date.dt.day\n",
    "\n",
    "submit['week_start_date'] = pd.to_datetime(submit['week_start_date'], format='%Y-%m-%d')\n",
    "submit['quarter'] = submit.week_start_date.dt.quarter\n",
    "submit['month'] = submit.week_start_date.dt.month\n",
    "submit['day'] = submit.week_start_date.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna via naive method, median, grouped by city and month\n",
    "training = training.groupby(['city', 'month'], as_index=False).apply(lambda x: x.fillna(x.median())).reset_index(drop=True)\n",
    "submit = submit.groupby(['city', 'month'], as_index=False).apply(lambda x: x.fillna(x.median())).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some highly correlated non-datetime related features (absolute correlation > 0.9)\n",
    "features_to_remove = ['reanalysis_sat_precip_amt_mm', 'reanalysis_specific_humidity_g_per_kg', 'reanalysis_avg_temp_k', 'reanalysis_tdtr_k']\n",
    "training.drop(features_to_remove, axis=1, inplace=True)\n",
    "submit.drop(features_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for categorical data: city\n",
    "training = pd.concat([pd.get_dummies(training.city).astype(int), training], axis=1)\n",
    "submit = pd.concat([pd.get_dummies(submit.city).astype(int), submit], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: train a model and test result simply using training data\n",
    "\n",
    "This step helps us understand the accuracy of our model before using it to predict the 'submit' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop week_start_date before training\n",
    "training.drop(['week_start_date', 'year'], axis=1, inplace=True)\n",
    "submit.drop(['week_start_date', 'year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for train, test and cv and then drop categorical variable: city\n",
    "X_train, X_test, y_train, y_test = train_test_split(training.drop(['total_cases'], axis=1), training.total_cases, test_size=0.2, stratify=training.city, random_state=123)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.25, stratify=X_train.city, random_state=123)\n",
    "\n",
    "X_train.drop('city', axis=1, inplace=True)\n",
    "X_cv.drop('city', axis=1, inplace=True)\n",
    "X_test.drop('city', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = CatBoostRegressor(iterations = 4000, learning_rate = 0.3, loss_function='MAE', eval_metric='MAE', use_best_model=True, random_seed=123)\n",
    "model.fit(X_train, y_train, eval_set=(X_cv, y_cv), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "y_pred = model.predict(X_test)\n",
    "print('Mean Absolute Error:', MAE(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe features importance\n",
    "pd.DataFrame(data={'features':X_train.columns, 'importance':model.feature_importances_}).plot.bar(x='features', y='importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: train the model again using all training data and do prediction for the 'submit' set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for train, and cv\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(training.drop(['total_cases'], axis=1), training.total_cases, test_size=0.3, stratify=training.city, random_state=123)\n",
    "X_train.drop('city', axis=1, inplace=True)\n",
    "X_cv.drop('city', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = CatBoostRegressor(iterations = 4000, learning_rate = 0.3, loss_function='MAE', eval_metric='MAE', use_best_model=True, random_seed=123)\n",
    "model.fit(X_train, y_train, eval_set=(X_cv, y_cv), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction on the 'submit' data and save as csv for final submission\n",
    "submit_Y.total_cases = model.predict(submit.drop(['total_cases', 'city'], axis=1)).astype(int)\n",
    "submit_Y.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
